\section{Evaluation}

% how the wsd proposed in this paper works:
% 	1. language model from wikipedia corpus
% 		-LDA & LSA
% 		-different dimensionalities
% 	2. labeled reference data
% 		-convert documents of reference data to topic vector space
% 		-save vector with sense id (=label)
% 	3. sense disambiguation
% 		-convert tested document to topic vector space
% 		-retrieve top k senseid using cosine measure from saved vector->senseid mapping
% 		-sense id of tested document is most frequent sense id in the top k retrieved


We are creating language models on the english wikipedia corpus (see \ref{wikidump}). As mentioned before, we are creating LDA and LSA topic models. When creating the models the number of extracted topics has to be set a priori. For investigating the influence of topic count on the results, we are creating multiple instance of LDA and LSA models with different amounts of topics.\\
We are provided with a training set containing documents labeled with a sense id. Those documents are converted to the vector space of the topic model. Every entry $i$ of the vector then represents how much the document relates to the topic $i$ of the model. The vector is then saved associated with the sense id of the document.\\
The sense disambiguation of a test document is done as follows. The test document is converted to the vector space of the topic model. The top $k$ documents of the saved vector to sense id is retrieved using the cosine measure. The sense id of the test document is the most frequent sense of the retreived vectors.



\subsection{Data}
\label{wikidump}
% details on data and how we created topic models
% 	1. wikipedia dumps
% 		-wikipedia does frequent dumps
% 		-we are using the latest dump of the articles in english wikipedia (as of june 2014) as xml
% 		-about 10GiB of compressed data, ~3.6 million documents
% 	2. topic model creationg
% 		-using gensim python library (decription of library)
% 		-gensim usage: lsa/lda interface, paramter description
% 		-description of calculation time and machine hardware
% 	3. additional models
% 		-hdp as test, implementation not mature therefore just used as test case
% 		-other models provided by gensim (random projections, log entropy) not used



\subsection{Scoring}
% how did we score the diffent models
% 	1. scoring script provided, runs on test set, gives f-score/recall
% 	2. plotting tools / how were the graphics generated
% 	3. plot data: dimensionality against f-score
% 	4. interpretation of plots: what does that mean, how well did it perform


