\section{Evaluation}

% how the wsd proposed in this paper works:
% 	1. language model from wikipedia corpus
% 		-LDA & LSA
% 		-different dimensionalities
% 	2. labeled reference data
% 		-convert documents of reference data to topic vector space
% 		-save vector with sense id (=label)
% 	3. sense disambiguation
% 		-convert tested document to topic vector space
% 		-retrieve top k senseid using cosine measure from saved vector->senseid mapping
% 		-sense id of tested document is most frequent sense id in the top k retrieved


We are creating language models on the english wikipedia corpus (see \ref{creation}). As mentioned before, we are creating LDA and LSA topic models. When creating the models the number of extracted topics has to be set a priori. For investigating the influence of topic count on the results, we are creating multiple instance of LDA and LSA models with different amounts of topics.\\
We are provided with a training set containing documents labeled with a sense id. Those documents are converted to the vector space of the topic model. Every entry $i$ of the vector then represents how much the document relates to the topic $i$ of the model. The vector is then saved associated with the sense id of the document.\\
The sense disambiguation of a test document is done as follows. The test document is converted to the vector space of the topic model. The top $k$ documents of the saved vector to sense id is retrieved using the cosine measure. The sense id of the test document is the most frequent sense of the retreived vectors.



\subsection{Creation of Topic Models}
\label{creation}
% details on data and how we created topic models
% 	1. wikipedia dumps
% 		-wikipedia does frequent dumps
% 		-we are using the latest dump of the articles in english wikipedia (as of june 2014) as xml
% 		-about 10GiB of compressed data, ~3.6 million documents
% 	2. topic model creationg
% 		-using gensim python library (decription of library)
% 		-gensim usage: lsa/lda interface, paramter description
% 		(-description of calculation time and machine hardware)
% 	3. additional models
% 		-hdp as test, implementation not mature therefore just used as test case
% 		-other models provided by gensim (random projections, log entropy) not used

For the training of a topic model large amounts of text are required. For providing that data we looked at the english Wikipedia. Wikipedia does frequent backups of its' articles and provides them for public use. We used the latest dump (as of june 2014) containing about 10GiB of compressed xml data or 3.6 million documents \cite{wikidumps}. The topic model was then trained using that data in an unsupervised manner.\\
The implementation of th LSA and LDA topic models we used was provided by the python library gensim. Gensim is a free, open-source library for topic modelling. For using the wikipedia dump with gensim, it first had to be preprocessed using TF-IDF. Gensim was than used to create LSA and LDA models with severel dimensionalities from that corpus.\\
There are other topic models that are also supported by gensim that were not used for different reasons. Random projections \cite{RandomProjections, gensimRP} and log entropy \cite{LogEntropy, gensimLE} models were not used due to time contraints. We did create a hierarchical dirichlet process \cite{hdp, gensimHdp} that did not work properly. Since the implementation of that model is not mature \cite{gensimHdp} we did not investigate the failiure further.





\subsection{Results}
We ran a large number of experiments at different parameters in order to find the optimal model and configuration. LSA was evaluated at 150, 200, 250, 300, 350 and 400 topics, each with a single pass over the corpus, two power iterations and no decay.\\ 
LDA was only evaluated at 200, 300 and 400 topics, since it is more computationally expensive. Here we used a symmetric prior of $\alpha = \frac{1}{n}$ for the per-document topic distribution, with the same value for $\beta$, the per-topic word distribution.\\
HDP was evaluated using the default parameters of Gensim: $\kappa = 1, \tau = 64, \alpha =1, \gamma =1, \eta =0.01$. Unfortunately, we were not able to use our HDP model, as the topics it did not make sense, and were all nearly identical. Due to time constraints we were not able to fix these issues and train another model.\\\\
The plot below visualizes the results we obtained for different dimensionalities 
% how did we score the diffent models
% 	1. scoring script provided, runs on test set, gives f-score/recall
% 	2. plotting tools / how were the graphics generated
% 	3. plot data: dimensionality against f-score
% 	4. interpretation of plots: what does that mean, how well did it perform


