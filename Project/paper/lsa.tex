\section{Topic Models}
Our experiments dealt with three different topic models: Latent Semantic Analysis, Latent Dirichlet Allocation and the Hierarchical Dirichlet Process. For more detailed description of each of these models see \cite{LSA_paper} \cite{LDA_paper} \cite{HDP_paper}.
\subsection{Latent Semantic Analysis}

Latent Semantic Analysis (LSA) is a method that allows to identify topcis in unstructured text. It uses the bag of words model and the notion that words with similar meaning will occur in similar pieces of text. \\
When using LSA, one would typically construct a matrix $C$  with each row representing a unique word and each column representing a document or paragraph. The entry $C_{i,j}$ would then contain the number of occurrences of the (unique) word $i$ in the document $j$. In our particular implementation we chose to use the TF-IDF frequency instead of plain word counts, in an attempt to boost the relevance of rare words. The matrix $C$ will invariably be very sparse as each document only contains a tiny fraction of the total number of words. Furthermore, with large numbers of documents as input the matrix will also be of very high rank and simply be too massive to process or even fit in memory. LSA thus uses a mathematical technique called Singular Value Decomposition (SVD) to reduce the rank of the matrix to $k$, with $k$ usually being in the low hundreds. In short, the SVD of a matrix $C$ leads to three matrices so that $C = U{\Sigma}V^T$ is true, with $U$ and $V$ being orthogonal matrices and $\Sigma$ being a diagonal matrix. The entries $\Sigma_{i,i}$ are called the singular values with $U_{i}$ and $V_{i}$ being the left and right singular vectors respectively. The dimensionality reduction to rank $k$ then works by selecting the $k$ biggest singular values and the corresponding singular vectors $C_{k} = U_{k}{\Sigma_{k}}V_{k}$. This leads to a mathematical approximation of rank $k$ that has minimal error.\\\\

The resulting low dimensionality matrix $C_{k}$ is now referred to as \emph{semantic space} and finding similar words/topics boils down to computing the \emph{cosine distance} between two word vectors. A cosine value close to one means that the values are very similar (e.g. synonyms) while a value close to zero hints at words with different meaning.
